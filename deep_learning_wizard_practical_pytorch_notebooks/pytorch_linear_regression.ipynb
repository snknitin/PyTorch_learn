{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Linear Regression with PyTorch\n",
    "## 1. About Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simple Linear Regression Basics\n",
    "- Allows us to understand **relationship** between two **continuous variables**\n",
    "- Example\n",
    "    - x: independent variable\n",
    "        - weight\n",
    "    - y: dependent variable\n",
    "        - height\n",
    "- $y = \\alpha x + \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Example of simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEELUmf8fNDVU95s4cmMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd790b585c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)\n",
    "n = 50\n",
    "x = np.random.randn(n)\n",
    "y = x * np.random.randn(n)\n",
    "\n",
    "colors = np.random.rand(n)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Aim of Linear Regression\n",
    "- Minimize the distance between the points and the line ($y = \\alpha x + \\beta$)\n",
    "- Adjusting\n",
    "    - Coefficient: $\\alpha$\n",
    "    - Bias/intercept: $\\beta$\n",
    "\n",
    "## 2. Building a Linear Regression Model with PyTorch\n",
    "\n",
    "### 2.1 Example\n",
    "- Coefficient: $\\alpha = 2$\n",
    "- Bias/intercept: $\\beta = 1$\n",
    "- Equation: $y = 2x + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_values = [i for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: 2D required\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = 2x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_values = [2*i + 1 for i in x_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case you're weak in list iterators...\n",
    "y_values = []\n",
    "for i in x_values:\n",
    "    result = 2*i + 1\n",
    "    y_values.append(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: 2D required\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Model**\n",
    "1. Linear model\n",
    "    - True Equation: $y = 2x + 1$\n",
    "2. Forward\n",
    "    - Example\n",
    "        - Input $x = 1 $\n",
    "        - Output $\\hat y = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate Model Class**\n",
    "- input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate Loss Class**\n",
    "- MSE Loss: Mean Squared Error\n",
    "- $MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)$\n",
    "    - $\\hat y$: prediction\n",
    "    - $y$: true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate Optimizer Class**\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
    "        - $\\theta$: parameters (our variables)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "        - parameters: $\\alpha$ and $\\beta$ in $ y = \\alpha x + \\beta$\n",
    "        - desired parameters: $\\alpha = 2$ and $\\beta = 1$ in $ y = 2x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**\n",
    "- 1 epoch: going through the whole x_train data once\n",
    "    - 100 epochs: \n",
    "        - 100x mapping `x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`\n",
    "        \n",
    "- Process \n",
    "    1. Convert inputs/labels to tensors with gradients\n",
    "    2. Clear gradient buffets\n",
    "    3. Get output given inputs \n",
    "    4. Get loss\n",
    "    5. Get gradients w.r.t. parameters\n",
    "    6. Update parameters using gradients\n",
    "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "    7. REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 5.488742351531982\n",
      "epoch 2, loss 0.9309054017066956\n",
      "epoch 3, loss 0.5537410974502563\n",
      "epoch 4, loss 0.5176412463188171\n",
      "epoch 5, loss 0.5094209313392639\n",
      "epoch 6, loss 0.503533124923706\n",
      "epoch 7, loss 0.4978938400745392\n",
      "epoch 8, loss 0.49233272671699524\n",
      "epoch 9, loss 0.4868347942829132\n",
      "epoch 10, loss 0.4813983738422394\n",
      "epoch 11, loss 0.4760226905345917\n",
      "epoch 12, loss 0.4707070291042328\n",
      "epoch 13, loss 0.4654509425163269\n",
      "epoch 14, loss 0.46025320887565613\n",
      "epoch 15, loss 0.45511385798454285\n",
      "epoch 16, loss 0.45003125071525574\n",
      "epoch 17, loss 0.4450059235095978\n",
      "epoch 18, loss 0.4400363862514496\n",
      "epoch 19, loss 0.43512293696403503\n",
      "epoch 20, loss 0.43026426434516907\n",
      "epoch 21, loss 0.4254593551158905\n",
      "epoch 22, loss 0.4207083582878113\n",
      "epoch 23, loss 0.4160100221633911\n",
      "epoch 24, loss 0.41136452555656433\n",
      "epoch 25, loss 0.406770795583725\n",
      "epoch 26, loss 0.4022289216518402\n",
      "epoch 27, loss 0.3977372348308563\n",
      "epoch 28, loss 0.3932957351207733\n",
      "epoch 29, loss 0.38890397548675537\n",
      "epoch 30, loss 0.38456085324287415\n",
      "epoch 31, loss 0.38026687502861023\n",
      "epoch 32, loss 0.3760201334953308\n",
      "epoch 33, loss 0.37182101607322693\n",
      "epoch 34, loss 0.36766913533210754\n",
      "epoch 35, loss 0.363563597202301\n",
      "epoch 36, loss 0.35950368642807007\n",
      "epoch 37, loss 0.35548898577690125\n",
      "epoch 38, loss 0.3515195846557617\n",
      "epoch 39, loss 0.3475937843322754\n",
      "epoch 40, loss 0.34371262788772583\n",
      "epoch 41, loss 0.3398740589618683\n",
      "epoch 42, loss 0.3360789716243744\n",
      "epoch 43, loss 0.3323259651660919\n",
      "epoch 44, loss 0.32861506938934326\n",
      "epoch 45, loss 0.3249454200267792\n",
      "epoch 46, loss 0.3213166892528534\n",
      "epoch 47, loss 0.3177286386489868\n",
      "epoch 48, loss 0.3141808807849884\n",
      "epoch 49, loss 0.31067243218421936\n",
      "epoch 50, loss 0.3072032332420349\n",
      "epoch 51, loss 0.3037724792957306\n",
      "epoch 52, loss 0.30038049817085266\n",
      "epoch 53, loss 0.29702627658843994\n",
      "epoch 54, loss 0.29370906949043274\n",
      "epoch 55, loss 0.2904292047023773\n",
      "epoch 56, loss 0.28718623518943787\n",
      "epoch 57, loss 0.28397923707962036\n",
      "epoch 58, loss 0.28080809116363525\n",
      "epoch 59, loss 0.27767235040664673\n",
      "epoch 60, loss 0.27457162737846375\n",
      "epoch 61, loss 0.2715055048465729\n",
      "epoch 62, loss 0.26847389340400696\n",
      "epoch 63, loss 0.26547566056251526\n",
      "epoch 64, loss 0.26251134276390076\n",
      "epoch 65, loss 0.25957977771759033\n",
      "epoch 66, loss 0.2566811144351959\n",
      "epoch 67, loss 0.25381484627723694\n",
      "epoch 68, loss 0.2509804964065552\n",
      "epoch 69, loss 0.24817775189876556\n",
      "epoch 70, loss 0.24540619552135468\n",
      "epoch 71, loss 0.2426658719778061\n",
      "epoch 72, loss 0.23995620012283325\n",
      "epoch 73, loss 0.2372766137123108\n",
      "epoch 74, loss 0.23462699353694916\n",
      "epoch 75, loss 0.23200693726539612\n",
      "epoch 76, loss 0.22941619157791138\n",
      "epoch 77, loss 0.22685417532920837\n",
      "epoch 78, loss 0.2243211269378662\n",
      "epoch 79, loss 0.221816286444664\n",
      "epoch 80, loss 0.21933913230895996\n",
      "epoch 81, loss 0.21688981354236603\n",
      "epoch 82, loss 0.2144676297903061\n",
      "epoch 83, loss 0.21207267045974731\n",
      "epoch 84, loss 0.2097046822309494\n",
      "epoch 85, loss 0.2073630839586258\n",
      "epoch 86, loss 0.20504745841026306\n",
      "epoch 87, loss 0.20275749266147614\n",
      "epoch 88, loss 0.20049352943897247\n",
      "epoch 89, loss 0.19825458526611328\n",
      "epoch 90, loss 0.19604067504405975\n",
      "epoch 91, loss 0.19385138154029846\n",
      "epoch 92, loss 0.19168691337108612\n",
      "epoch 93, loss 0.18954643607139587\n",
      "epoch 94, loss 0.1874295175075531\n",
      "epoch 95, loss 0.18533669412136078\n",
      "epoch 96, loss 0.18326707184314728\n",
      "epoch 97, loss 0.1812206506729126\n",
      "epoch 98, loss 0.1791967749595642\n",
      "epoch 99, loss 0.17719584703445435\n",
      "epoch 100, loss 0.1752171665430069\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # Convert numpy array to torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.22133885],\n",
       "       [  2.33347297],\n",
       "       [  4.44560719],\n",
       "       [  6.55774164],\n",
       "       [  8.6698761 ],\n",
       "       [ 10.78201008],\n",
       "       [ 12.89414501],\n",
       "       [ 15.00627899],\n",
       "       [ 17.11841202],\n",
       "       [ 19.23054695],\n",
       "       [ 21.34267998]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Purely inference\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.],\n",
       "       [  3.],\n",
       "       [  5.],\n",
       "       [  7.],\n",
       "       [  9.],\n",
       "       [ 11.],\n",
       "       [ 13.],\n",
       "       [ 15.],\n",
       "       [ 17.],\n",
       "       [ 19.],\n",
       "       [ 21.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = 2x + 1 \n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niP8P/V5Qsr4DKSkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd76e6b4358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear figure\n",
    "plt.clf()\n",
    "\n",
    "# Get predictions\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "# Plot true data\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "\n",
    "# Legend and plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model is True:\n",
    "    # Saves only parameters\n",
    "    # alpha & beta\n",
    "    torch.save(model.state_dict(), 'awesome_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model is True:\n",
    "    model.load_state_dict(torch.load('awesome_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Linear Regression Model with PyTorch (GPU)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CPU Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "STEP 1: CREATE MODEL CLASS\n",
    "'''\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 2: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 3: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 5: TRAIN THE MODEL\n",
    "'''\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # Convert numpy array to torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU: 2 things must be on GPU\n",
    "- `model`\n",
    "- `tensors with gradients`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 311.9888000488281\n",
      "epoch 2, loss 25.992170333862305\n",
      "epoch 3, loss 2.658254861831665\n",
      "epoch 4, loss 0.7489710450172424\n",
      "epoch 5, loss 0.5872949957847595\n",
      "epoch 6, loss 0.5682318210601807\n",
      "epoch 7, loss 0.5608657598495483\n",
      "epoch 8, loss 0.5545195937156677\n",
      "epoch 9, loss 0.5483205914497375\n",
      "epoch 10, loss 0.5421971678733826\n",
      "epoch 11, loss 0.5361425280570984\n",
      "epoch 12, loss 0.5301553606987\n",
      "epoch 13, loss 0.5242351293563843\n",
      "epoch 14, loss 0.5183811783790588\n",
      "epoch 15, loss 0.5125927329063416\n",
      "epoch 16, loss 0.506868302822113\n",
      "epoch 17, loss 0.5012083649635315\n",
      "epoch 18, loss 0.4956115484237671\n",
      "epoch 19, loss 0.49007731676101685\n",
      "epoch 20, loss 0.4846043288707733\n",
      "epoch 21, loss 0.4791930317878723\n",
      "epoch 22, loss 0.4738418459892273\n",
      "epoch 23, loss 0.4685504138469696\n",
      "epoch 24, loss 0.4633180797100067\n",
      "epoch 25, loss 0.4581444561481476\n",
      "epoch 26, loss 0.4530283212661743\n",
      "epoch 27, loss 0.447969526052475\n",
      "epoch 28, loss 0.44296738505363464\n",
      "epoch 29, loss 0.4380207359790802\n",
      "epoch 30, loss 0.43312931060791016\n",
      "epoch 31, loss 0.42829257249832153\n",
      "epoch 32, loss 0.4235101044178009\n",
      "epoch 33, loss 0.4187808036804199\n",
      "epoch 34, loss 0.4141041040420532\n",
      "epoch 35, loss 0.40947991609573364\n",
      "epoch 36, loss 0.40490731596946716\n",
      "epoch 37, loss 0.4003855586051941\n",
      "epoch 38, loss 0.39591485261917114\n",
      "epoch 39, loss 0.3914933502674103\n",
      "epoch 40, loss 0.38712164759635925\n",
      "epoch 41, loss 0.382798969745636\n",
      "epoch 42, loss 0.3785242736339569\n",
      "epoch 43, loss 0.37429723143577576\n",
      "epoch 44, loss 0.37011778354644775\n",
      "epoch 45, loss 0.36598464846611023\n",
      "epoch 46, loss 0.36189764738082886\n",
      "epoch 47, loss 0.35785675048828125\n",
      "epoch 48, loss 0.3538605868816376\n",
      "epoch 49, loss 0.34990885853767395\n",
      "epoch 50, loss 0.34600162506103516\n",
      "epoch 51, loss 0.34213787317276\n",
      "epoch 52, loss 0.3383171856403351\n",
      "epoch 53, loss 0.3345392048358917\n",
      "epoch 54, loss 0.33080336451530457\n",
      "epoch 55, loss 0.32710957527160645\n",
      "epoch 56, loss 0.3234565854072571\n",
      "epoch 57, loss 0.31984472274780273\n",
      "epoch 58, loss 0.316273033618927\n",
      "epoch 59, loss 0.3127409815788269\n",
      "epoch 60, loss 0.30924874544143677\n",
      "epoch 61, loss 0.3057953417301178\n",
      "epoch 62, loss 0.30238085985183716\n",
      "epoch 63, loss 0.29900428652763367\n",
      "epoch 64, loss 0.29566508531570435\n",
      "epoch 65, loss 0.29236340522766113\n",
      "epoch 66, loss 0.28909897804260254\n",
      "epoch 67, loss 0.2858702838420868\n",
      "epoch 68, loss 0.28267809748649597\n",
      "epoch 69, loss 0.2795216143131256\n",
      "epoch 70, loss 0.27639999985694885\n",
      "epoch 71, loss 0.2733135223388672\n",
      "epoch 72, loss 0.27026137709617615\n",
      "epoch 73, loss 0.2672436237335205\n",
      "epoch 74, loss 0.26425933837890625\n",
      "epoch 75, loss 0.2613084018230438\n",
      "epoch 76, loss 0.25839054584503174\n",
      "epoch 77, loss 0.2555052638053894\n",
      "epoch 78, loss 0.2526516616344452\n",
      "epoch 79, loss 0.24983063340187073\n",
      "epoch 80, loss 0.24704064428806305\n",
      "epoch 81, loss 0.24428211152553558\n",
      "epoch 82, loss 0.24155420064926147\n",
      "epoch 83, loss 0.238856703042984\n",
      "epoch 84, loss 0.23618966341018677\n",
      "epoch 85, loss 0.23355217278003693\n",
      "epoch 86, loss 0.2309437245130539\n",
      "epoch 87, loss 0.22836481034755707\n",
      "epoch 88, loss 0.22581501305103302\n",
      "epoch 89, loss 0.22329331934452057\n",
      "epoch 90, loss 0.22079992294311523\n",
      "epoch 91, loss 0.21833394467830658\n",
      "epoch 92, loss 0.21589598059654236\n",
      "epoch 93, loss 0.21348515152931213\n",
      "epoch 94, loss 0.21110118925571442\n",
      "epoch 95, loss 0.20874366164207458\n",
      "epoch 96, loss 0.20641273260116577\n",
      "epoch 97, loss 0.20410780608654022\n",
      "epoch 98, loss 0.20182862877845764\n",
      "epoch 99, loss 0.199574813246727\n",
      "epoch 100, loss 0.1973460614681244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "STEP 1: CREATE MODEL CLASS\n",
    "'''\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 2: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "'''\n",
    "STEP 3: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 5: TRAIN THE MODEL\n",
    "'''\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # Convert numpy array to torch Variable\n",
    "    \n",
    "    #######################\n",
    "    #  USE GPU FOR MODEL  #\n",
    "    #######################\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "\n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # Forward to get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Logging\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple **linear regression basics**\n",
    "    - $y = Ax + B$\n",
    "    - $y = 2x + 1$\n",
    "- **Example** of simple linear regression\n",
    "- **Aim** of linear regression\n",
    "    - Minimizing distance between the points and the line\n",
    "        - Calculate \"distance\" through `MSE`\n",
    "        - Calculate `gradients`\n",
    "        - Update parameters with `parameters = parameters - learning_rate * gradients`\n",
    "        - Slowly update parameters $A$ and $B$ model the linear relationship between $y$ and $x$ of the form $y = 2x + 1$\n",
    "- Built a linear regression **model** in **CPU and GPU**\n",
    "    - Step 1: Create Model Class\n",
    "    - Step 2: Instantiate Model Class\n",
    "    - Step 3: Instantiate Loss Class\n",
    "    - Step 4: Instantiate Optimizer Class\n",
    "    - Step 5: Train Model\n",
    "- Important things to be on **GPU**\n",
    "    - `model`\n",
    "    - `tensors with gradients`\n",
    "- How to bring to **GPU**?\n",
    "    - `model_name.cuda()`\n",
    "    - `variable_name.cuda()`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
